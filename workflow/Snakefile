############################################################
#
# General introduction
# ====================
#
# This Snakefile coordinates the execution of the following, 
# among other things:
#
#  - Download and preprocessing of datasets
#  - Generation of synthetic query workloads under different
#    parameterizations
#  - Plotting of the resulting difficulty metrics
#
# This file works a lot like a Makefile with wildcard
# patterns. In particular, there are several `rule` blocks,
# each defining its input and output files. Snakemake then
# infers the dependencies between tasks by matching file
# names. Then, file names can have "wildcards" in them, i.e.
# placeholders that will be made available to rule bodies
# (marked with the `run` block) as parameters.
#
# The execution therefore proceeds lazily. If we run, on 
# the command line
#
#     snakemake --cores all plot_syntetic_difficulty
#
# then snakemake will look at any missing input of the
# plot_syntetic_difficulty rule, and compute it, possibly
# computing any other missing file recursively, down to the
# input files.
#
# Rules in Snakemake (rather, their `run` block) can be
# written directly in Python

############################################################
#
# General setup
#
############################################################

# Configuration of the data directory. Setting the environment variable
# WORKGEN_DATA_DIR allows to specify a directory where to look
# for files and where to save preprocessed ones. The default
# is a hidden directory in the current working directory.
import os
DATA_DIR = os.environ.get("WORKGEN_DATA_DIR", ".data") # on nefeli: export WORKGEN_DATA_DIR=/mnt/hddhelp/workgen_data/
GENERATED_DIR = os.path.join(DATA_DIR, "generated")
RES_DIR = os.environ.get("WORKGEN_RES_DIR", "results") # on nefeli: export WORKGEN_RES_DIR=/mnt/hddhelp/workgen_results/
MESSI_DIR = os.environ.get("MESSI_INSTALL_DIR", "/home/qwang/projects/isax-modularized/build/")

# Here we list the minimum accepted snakemake version
from snakemake.utils import min_version
min_version("8.4")

# We can import other packages. One cool thing of Snakemake
# is that the entire Snakefile can be scripted with Python
import pandas as pd
from parameters import workloads

# All the workloads considered in our experiments. 
# See the `parameters` module.
WORKLOADS= workloads()

configfile: "workflow/config.yaml"

rule all:
    input:
        os.path.join(RES_DIR, "plots", "all-metrics.png"),
        os.path.join(RES_DIR, "plots", "index-performance.png"),
        os.path.join(RES_DIR, "plots", "correlations-bars.png"),
        os.path.join(RES_DIR, "plots", "sota-distcomps.png"),
        os.path.join(RES_DIR, "metrics", "index-performance.csv")

######################################################################
#
# Workload generation
#
######################################################################

# Here we generate workloads sets
rule generate_workload:
    input:
        os.path.join(DATA_DIR, "{dataset}.bin")
    output:
        os.path.join(GENERATED_DIR,
          f"{WORKLOADS.wildcard_pattern}.bin"
        ),
    threads: workflow.cores
    run:
        import workflow.parameters as wparams
        import generate
        import logging
        import sys
        import os
        logging.basicConfig(level=logging.DEBUG)

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        workload_type = workload_config["workload_type"]
        if workload_type == "synthetic-simulated-annealing":
            generate.generate_workload_annealing(
              input[0],
              output[0],
              k = int(workload_config['k']),
              metric = workload_config["difficulty"],
              target_class = workload_config['target_class'],
              num_queries = int(workload_config['num_queries']),
              threads = threads,
              max_rounds = "return-any"
            )
        elif workload_type == "synthetic-sgd":
            generate.generate_workload_sgd(
              input[0],
              output[0],
              k = int(workload_config['k']),
              target_class = workload_config['target_class'],
              num_queries = int(workload_config['num_queries']),
              threads = threads,
            )
        elif workload_type == "synthetic-gaussian-noise":
            print("generating with gaussian noise", workload_config)
            generate.generate_workload_gaussian_noise(
                input[0],
                output[0],
                num_queries=workload_config["num_queries"],
                scale=workload_config["scale"],
            )
        elif workload_type == "file-based":
            print("linking file-based workload")
            os.symlink(
              os.path.abspath(
                os.path.join(DATA_DIR, workload_config["queries_filename"]) + ".bin"
              ),
              output[0]
            )
        else:
            raise ValueError(f"unknown workload type {workload_type}")


# And finally here we compute the metrics of the synthetic
# query sets
rule eval_metrics:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        queries =
          os.path.join(GENERATED_DIR, 
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
    output:
        os.path.join(RES_DIR, f"metrics/{WORKLOADS.wildcard_pattern}.csv"),
    threads: workflow.cores
    run:
        import workflow.parameters as wparams
        import metrics

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        workload_descr = WORKLOADS.description_for(wildcards["workload_key"])

        metrics.metrics_csv(
          input['data'],
          input['queries'],
          output[0],
          workload_config['k'],
          additional_header = ["dataset", "workload_description"],
          additional_row = [workload_config['dataset'], workload_descr]
        )


# Here we bring in a single csv file the metrics computed
# on the syntehtic query sets
rule eval_metrics_concat:
    input:
        metrics = expand(
          os.path.join(RES_DIR, "metrics", "{params}.csv"),
          params=WORKLOADS.instance_patterns
        )
    output:
        #"results/metrics/all-metrics.csv"
        os.path.join(RES_DIR, "metrics","all-metrics.csv")
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input["metrics"]))
        df.to_csv(output[0], index=False)

# This rule plots the number of distance computations against
# the RC metric for different datasets, coloring points
# according to their "difficulty group"
rule plot_difficulty:
    input:
        #metrics = "results/metrics/all-metrics.csv"
        metrics = os.path.join(RES_DIR, "metrics","all-metrics.csv")
    output:
        #"results/plots/all-metrics.png"
        os.path.join(RES_DIR, "plots","all-metrics.png")
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input["metrics"])
        print(df)
        averages = df.groupby(["dataset", "workload_description"]).mean().reset_index()

        (
          so.Plot(df,
                  y="distcomp",
                  x="workload_description",
                  color="workload_description")
            .facet("dataset")
            .add(so.Dot())
            .save(output[0])
        )

rule plot_distcomps:
    input:
        metrics = os.path.join(RES_DIR, "metrics","all-metrics.csv")
    output:
        os.path.join(RES_DIR, "plots", "sota-distcomps.png")
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input["metrics"])
        df = df[df["workload_description"].str.startswith("File")]

        sns.violinplot(
            data = df,
            x = "distcomp",
            y = "dataset",
            split = True,
            inner = None,
            cut = 0,
        )
        plt.tight_layout()
        plt.savefig(output[0])
        #(
        #  so.Plot(df,
        #          x="distcomp")
        #    .add(so.Area(), so.KDE(cut=0), color="dataset")
        #    .facet(row="dataset")
        #    .share(y=False)
        #    .layout(size=(6,12))
        #    .save(output[0])
        #)

######################################################################
#
# Indices
#
######################################################################

rule plot_index_performance:
    input:
        metrics = os.path.join(RES_DIR, "metrics","index-performance.csv")
    output:
        os.path.join(RES_DIR, "plots","index-performance.png")
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        def get_family(c):
            if c.startswith("Gaussian"):
                return "GaussianNoise"
            if c.startswith("Annealing(faiss_ivf"):
                return "Annealing(faiss_ivf)"
            if c.startswith("Annealing(rc"):
                return "Annealing(rc)"
            return c

        df = pd.read_csv(input["metrics"])
        df.replace(
            {
                "GaussianNoise(0.1)": "GaussianNoise(easy)",
                "GaussianNoise(1.0)": "GaussianNoise(medium)",
                "GaussianNoise(10.0)": "GaussianNoise(difficult)",
                "Annealing(rc, 2.10, 1.90)": "Annealing(rc, easy)",
                "Annealing(rc, 1.80, 1.70)": "Annealing(rc, medium)",
                "Annealing(rc, 1.50, 1.30)": "Annealing(rc, difficult)",
                "Annealing(rc, 1.05, 1.03)": "Annealing(rc, medium)",
                "Annealing(rc, 1.01, 1.00)": "Annealing(rc, difficult)",
                "Annealing(faiss_ivf, 0.04, 0.06)": "Annealing(faiss_ivf, easy)",
                "Annealing(faiss_ivf, 0.19, 0.21)": "Annealing(faiss_ivf, medium)",
            },
            inplace=True
        )
        df.replace("File.*", "File", regex=True, inplace=True)
        df["family"] = df["workload"].map(get_family)
        # select the fastest configuration
        #df = df.iloc[df.groupby(["dataset", "workload", "query_index", "index_name", "target_recall"])["distcomp"].idxmin()]
        (
          so.Plot(df,
                  x="distcomp",
                  y="workload",
                  color="family")
            .facet("dataset", "index_name")
            .share(x=False)
            .add(so.Dot(), so.Jitter(0.2))
            .add(so.Dot(marker="*", pointsize=10, color="black"), so.Agg())
            .layout(size=(25, 10))
            .save(output[0])
        )

rule index_performance:
    input:
        expand(
            os.path.join(RES_DIR, "metrics/{index}/{pattern}/results.csv")    ,
            index = ["messi", "faiss_hnsw", "faiss_ivf"],
            pattern = WORKLOADS.instance_patterns
        )
    output:
        os.path.join(RES_DIR, "metrics", "index-performance.csv")
    run:
        import pandas as pd
        dfs = []
        for csv in input:
            dfs.append( pd.read_csv(csv) )
        pd.concat(dfs).to_csv(output[0])

rule run_messi_shell:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
    output:
        os.path.join(
          RES_DIR, 
          f"metrics/messi/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    log:
         os.path.join(RES_DIR,f"logs/messi/{WORKLOADS.wildcard_pattern}.log")
    run:
        import read_data as rd
        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        _, data_samples, slength, _ = rd.parse_filename(wildcards["dataset"])
        _, q_samples, q_slength, _ = rd.parse_filename(wildcards["workload_file"])
        assert slength == q_slength
        data_samples = min(data_samples, rd.MAX_DATA_LEN)
        print("Loading the first", data_samples, "vectors for MESSI")

        # how much cheaper is a SAX distance computation wrt a full distance
        # computation. We use this to aggregate the two types of distances into
        # a single one
        sax_rescale = config["sax_length"] / slength

        # FIXME: how dow we get the running time per query?

        try:
            os.remove(log[0])
        except FileNotFoundError:
            pass

        shell(MESSI_DIR + "/isax \
--database_filepath='{input.data}' \
--query_filepath='{input.workload}' \
--database_size={data_samples} \
--query_size={q_samples} \
--sax_length={config[sax_length]} \
--sax_cardinality={config[sax_card]} \
--cpu_cores={config[cpu_cores]} \
--log_filepath='{log}' \
--series_length={slength} \
--adhoc_breakpoints \
--leaf_size={config[leaf_size]} \
--k={k} \
--exact_search \
--with_id ")

        res = []
        for line_idx, log_line in enumerate(shell("grep -F 'l2square' {log} | grep 'query_engine.c' | cut -d ' ' -f 6,8,11", read=True, iterable=True)):
            q_idx, dists, sax_dist = list(map(int, log_line.split()))
            # distcomp = dists + sax_rescale * sax_dist
            distcomp = dists
            distcomp = distcomp / data_samples

            alg_name = "messi"
            recall = 1.0

            if q_idx >= q_samples:
                alg_name = "messi_apprx"
                q_idx -= q_samples
                recall = None

            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": alg_name,
                "index_params": "",
                "time_s": None,
                "distcomp": distcomp,
                "recall": recall,
            })
        try:
            os.remove(output[0])
        except FileNotFoundError:
            pass

        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)


rule run_faiss_hnsw:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
    output:
        os.path.join(
          RES_DIR, 
          f"metrics/faiss_hnsw/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    run:
        import read_data
        import faiss
        import utils
        import time
        import pandas as pd
        import json
        import indices
        from indices import FAISS_LOCK

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        dataset, distance_metric = read_data.read_multiformat(input["data"], "train")
        queryset, _ = read_data.read_multiformat(input["workload"], "test")
        print("Populating index")
        index_params = "HNSW32"
        index = indices.build_faiss_hnsw(dataset, index_params)
        print("done")
        target_recall = 0.95

        res = []

        for q_idx in range(queryset.shape[0]):
            print("running query", q_idx)
            q = queryset[q_idx,:].reshape(1, -1)
            ground_distances = utils.compute_distances(q, None, distance_metric, dataset)[0, :]
            index.hnsw.efSearch = 1
            rec = 0
            n_dist_comp = 0
            while rec < target_recall and index.hnsw.efSearch < 10**6:
                with FAISS_LOCK:
                    faiss.cvar.hnsw_stats.reset()
                    print("  efSearch", index.hnsw.efSearch)
                    dists = utils.compute_distances(q, k, distance_metric, index)[0, :]
                    rec = utils.compute_recall(ground_distances, dists, k)
                    if rec >= target_recall or index.hnsw.efSearch > 10**6:
                        stats = faiss.cvar.hnsw_stats
                        n_dist_comp = stats.n1 + stats.n2 + stats.n3 + stats.ndis
                        break
                # we only increment it if we are going to do another iteration
                index.hnsw.efSearch *= 2
            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": "faiss_hnsw",
                "target_recall": target_recall,
                "index_params": json.dumps({
                    "index_params": index_params,
                    "efSearch": index.hnsw.efSearch
                }, sort_keys=True),
                "distcomp": n_dist_comp / dataset.shape[0],
                "recall": rec,
            })
        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)


rule run_faiss_ivf:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
    output:
        os.path.join(
          RES_DIR, 
          f"metrics/faiss_ivf/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    run:
        import numpy as np
        import read_data
        import faiss
        import utils
        import time
        import pandas as pd
        import json
        import indices
        from indices import FAISS_LOCK

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        dataset, distance_metric = read_data.read_multiformat(input["data"], "train")
        queryset, _ = read_data.read_multiformat(input["workload"], "test")
        print("Populating index")
        n_list = int(np.ceil(np.sqrt(dataset.shape[0])))
        index_params = f"IVF{n_list},Flat"
        index = indices.build_faiss_ivf(dataset, n_list)
        print("done")
        target_recall = 0.95

        res = []

        for q_idx in range(queryset.shape[0]):
            print("running query", q_idx)
            q = queryset[q_idx,:].reshape(1, -1)
            ground_distances = utils.compute_distances(q, None, distance_metric, dataset)[0, :]
            index.nprobe = 1
            rec = 0
            n_dist_comp = 0
            while rec < target_recall and index.nprobe < 10**6:
                with FAISS_LOCK:
                    faiss.cvar.indexIVF_stats.reset()
                    print("  nprobe", index.nprobe)
                    dists = utils.compute_distances(q, k, distance_metric, index)[0, :]
                    rec = utils.compute_recall(ground_distances, dists, k)
                    if rec >= target_recall or index.nprobe > 10**6:
                        stats = faiss.cvar.indexIVF_stats
                        n_dist_comp = stats.ndis
                        break
                # we only increment it if we are going to do another iteration
                index.nprobe *= 2
            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": "faiss_ivf",
                "target_recall": target_recall,
                "index_params": json.dumps({
                    "index_params": index_params,
                    "nprobe": index.nprobe
                }, sort_keys=True),
                "distcomp": n_dist_comp / dataset.shape[0],
                "recall": rec,
            })
        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)


############################################################
#
# Correlation assessment
#
############################################################

rule correlation_analysis:
    input:
        os.path.join(RES_DIR, "metrics", "all-metrics.csv"),
    output:
        scatter = os.path.join(RES_DIR, "plots", "correlations.png"),
        correlations_bars = os.path.join(RES_DIR, "plots", "correlations-bars.png"),
        table = os.path.join(RES_DIR, "metrics", "correlations.csv"),
    run:
        import pandas as pd
        import numpy as np
        import seaborn.objects as so
        from scipy.stats import spearmanr, kendalltau

        def ts_fun(ys):
            for c in ys.columns:
                ys[c], bins = pd.qcut(ys[c], 100, labels=False, duplicates="drop", retbins=True)
            return ys.reset_index(drop=True)

        metrics = pd.read_csv(input[0])
        #metrics = metrics[metrics["workload_description"].str.startswith("File")]
        metrics.set_index("dataset", inplace=True)
        metrics = metrics[metrics.columns[metrics.dtypes == np.float64]]
        print(metrics)

        correlations = metrics.groupby(metrics.index).corr("kendall")
        correlations = correlations[["distcomp"]]
        correlations.reset_index(inplace=True)
        correlations.rename({"level_1": "measure", "distcomp": "distcomp_correlation"}, axis=1, inplace=True)
        correlations = correlations[correlations["measure"] != "distcomp"]
        correlations.to_csv(output["table"])
        correlations["sign"] = correlations["distcomp_correlation"] > 0
        correlations["distcomp_correlation"] = np.abs(correlations["distcomp_correlation"])

        df = metrics.reset_index().melt(
            value_vars=["lid_10", "rc_10", "exp_20|10", "eps_q0.25"], 
            id_vars=["distcomp", "dataset"]
        )
        
        (
          so.Plot(df,
                  y="distcomp",
                  x="value")
            .facet("variable", "dataset")
            .share(x=False)
            .scale(x="log", y="log")
            .add(so.Dots(pointsize=1))
            .layout(size=(16, 16))
            .save(output["scatter"])
        )

        (
          so.Plot(correlations,
                  x="distcomp_correlation",
                  y="measure",
                  color="sign")
            .facet(row="dataset")
            .add(so.Bars())
            .layout(size=(8, 12))
            .save(output["correlations_bars"])
        )

############################################################
#
# Data preparation
#
############################################################


rule prepare_data_file:
    output:
        os.path.join(DATA_DIR, "{name}.hdf5")
    wildcard_constraints:
        name="[a-zA-Z0-9-]+"
    run:
        import read_data
        name = wildcards["name"]
        read_data.download(f"https://ann-benchmarks.com/{name}.hdf5", output[0])


rule convert_annbench_to_bin:
    output:
        expand(
            os.path.join(DATA_DIR, "{name}.bin"),
            name=[
                "fashion_mnist-euclidean-784-60K",
                "glove-angular-32-1183514",
                "glove-angular-104-1183514",
                "nytimes-angular-256-289761",
            ]
        )
    run:
        import read_data
        features_map = {32: 25, 104: 100}
        for data_output in output:
            print("data output", data_output)
            name, samples, features, distance_metric = read_data.parse_filename(data_output)
            aname = name.replace("_", "-")
            ann_feats = features_map.get(features, features)
            ann_bench_name = f"{aname}-{ann_feats}-{distance_metric}"
            tmp = os.path.join(DATA_DIR, f"{ann_bench_name}.hdf5")
            read_data.download(f"https://ann-benchmarks.com/{ann_bench_name}.hdf5", tmp)
            read_data.hdf5_to_bin(tmp, data_output, "train")

            qsamples = read_data.read_multiformat(tmp, "test")[0].shape[0]
            queries_output = os.path.join(
                os.path.dirname(data_output),
                f"queries_{name}-{distance_metric}-{features}-{qsamples}.bin"
            )
            print("queries output", queries_output)
            read_data.hdf5_to_bin(tmp, queries_output, "test")


