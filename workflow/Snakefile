############################################################
#
# General introduction
# ====================
#
# This Snakefile coordinates the execution of the following, 
# among other things:
#
#  - Download and preprocessing of datasets
#  - Generation of synthetic query workloads under different
#    parameterizations
#  - Plotting of the resulting difficulty metrics
#
# This file works a lot like a Makefile with wildcard
# patterns. In particular, there are several `rule` blocks,
# each defining its input and output files. Snakemake then
# infers the dependencies between tasks by matching file
# names. Then, file names can have "wildcards" in them, i.e.
# placeholders that will be made available to rule bodies
# (marked with the `run` block) as parameters.
#
# The execution therefore proceeds lazily. If we run, on 
# the command line
#
#     snakemake --cores all plot_syntetic_difficulty
#
# then snakemake will look at any missing input of the
# plot_syntetic_difficulty rule, and compute it, possibly
# computing any other missing file recursively, down to the
# input files.
#
# Rules in Snakemake (rather, their `run` block) can be
# written directly in Python


############################################################
#
# General setup
#
############################################################

# Here we list the minimum accepted snakemake version
from snakemake.utils import min_version
min_version("7.24")

# We can import other packages. One cool thing of Snakemake
# is that the entire Snakefile can be scripted with Python
import pandas as pd
from parameters import setup_param_space

# This `paramspace` variable is a wrapper around a dataframe
# that holds all the parameter combinations we want to
# test. These parameter combinations are set up in the
# function in setup_param_space().
# More on this here: https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#parameter-space-exploration
paramspace = setup_param_space()
print(paramspace.dataframe)

# For convenience, here we extract the set of datasets we
# use in this evaluation.
DATASETS = set(paramspace.dataframe['dataset'])


# This rule plots the number of distance computations against
# the RC metric for different datasets, coloring points
# according to their "difficulty group"
rule plot_synthetic_difficulty:
    input:
        metrics = "results/metrics/all-synthetic.csv"
    output:
        "results/plots/synthetic-difficulty.png"
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input["metrics"])
        df["target_rc"] = df[[ "target_low", "target_high" ]].apply(lambda row: f"{row[0]}--{row[1]}", axis=1)
        averages = df.groupby(["dataset", "target_rc"]).mean().reset_index()

        (
          so.Plot(df, x="rc_10", y="distcomp", color="target_rc")
            .facet("dataset")
            .add(so.Dot())
            .add(so.Dot(color="black"), data=averages)
            .save(output[0])
        )

# This task computes the CSV file for the metrics
# of the queries builtin each dataset.
rule metrics_builtin_queries:
    input:
        expand(
          "results/metrics/{name}-k{k}.csv",
          name      = DATASETS,
          k         = [10]
        )
    output:
        "results/metrics/all-builtin.csv"
    threads: workflow.cores / 2
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


# Here we compute the metrics for each individual dataset, this
# a subtask of `metrics_builtin_queries`
rule eval_metrics_builtin_queries:
    input:
        ".data/{name}.hdf5"
    output:
        "results/metrics/{name}-k{k}.csv",
    run:
        import metrics
        metrics.metrics_csv(
          input[0],
          input[0],
          output[0],
          int(wildcards['k']),
          additional_header = ["dataset", "k"],
          additional_row = [wildcards['name'], int(wildcards['k'])]
        )


# Here we bring in a single csv file the metrics computed
# on the syntehtic query sets
rule synthetic_metrics:
    input:
        expand(
          "results/metrics/{params}.csv",
          params=paramspace.instance_patterns
        )
    output:
        "results/metrics/all-synthetic.csv"
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


# Here we generate syntehtic query sets, guided by the
# relative contrast metric
rule generate_rc:
    input:
        ".data/{dataset}.hdf5"
    output:
        f".generated/{paramspace.wildcard_pattern}.hdf5",
    threads: workflow.cores
    params:
        paramspace.instance
    run:
        import generate
        generate.generate_workload_annealing(
          input[0],
          output[0],
          k = int(wildcards['k']),
          metric = wildcards["difficulty"],
          target_low = float(wildcards['target_lower']),
          target_high = float(wildcards['target_upper']),
          num_queries = int(wildcards['num_queries']),
          scale = float(wildcards['scale']),
          initial_temperature = int(wildcards['initial_temperature']),
          max_steps = 10000,
          threads = threads
        )


# And finally here we compute the metrics of the synthetic
# query sets
rule eval_metrics:
    input:
        data = ".data/{dataset}.hdf5",
        queries = f".generated/{paramspace.wildcard_pattern}.hdf5"
    output:
        f"results/metrics/{paramspace.wildcard_pattern}.csv",
    params:
        conf = paramspace.instance
    run:
        import metrics
        metrics.metrics_csv(
          input['data'],
          input['queries'],
          output[0],
          int(wildcards['k']),
          additional_header = ["dataset", "target_low", "target_high"],
          additional_row = [wildcards['dataset'], float(wildcards['target_lower']), float(wildcards['target_upper'])]
        )

################################################################################
#
# UMAP Embeddings
# ===============
#
# This is still a draft, computing the UMAP embeddings for the
# datasets, to visualize them in two dimensions.
#
################################################################################

rule umap_embedding_plots:
    input:
        expand(".data/umap/{name}.png", name=DATASETS)

rule umap_embedding_plot:
    input:
        ".data/umap/{name}.hdf5"
    output:
        ".data/umap/{name}.png",
    run:
        import h5py
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt

        with h5py.File(input[0]) as hfp:
            embedding = hfp["train"][:]
        plt.scatter(embedding[:,0], embedding[:,1], s=0.1)
        plt.tight_layout()
        plt.savefig(output[0])


rule umap_embedding:
    input:
        ".data/{name}.hdf5"
    output:
        embedding = ".data/umap/{name}.hdf5",
        embedder = ".data/umap/embedder/{name}.pkl.gz"
    threads: workflow.cores
    run:
        import umap
        import read_data
        import h5py
        import pickle
        import gzip
        embedder = umap.UMAP()
        dataset, _ = read_data.read_hdf5(input[0], "train")
        embedding = embedder.fit_transform(dataset)
        with h5py.File(output["embedding"], "w") as hfp:
            hfp["train"] = embedding
        with gzip.open(output["embedder"], "wb") as fp:
            pickle.dump(embedder, fp)


rule prepare_data_file:
    output:
        ".data/{name}.hdf5"
    wildcard_constraints:
        name="[a-zA-Z0-9-]+"
    run:
        import read_data
        name = wildcards["name"]
        read_data.download(f"https://ann-benchmarks.com/{name}.hdf5", output[0])
