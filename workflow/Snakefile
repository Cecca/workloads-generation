############################################################
#
# General introduction
# ====================
#
# This Snakefile coordinates the execution of the following, 
# among other things:
#
#  - Download and preprocessing of datasets
#  - Generation of synthetic query workloads under different
#    parameterizations
#  - Plotting of the resulting difficulty metrics
#
# This file works a lot like a Makefile with wildcard
# patterns. In particular, there are several `rule` blocks,
# each defining its input and output files. Snakemake then
# infers the dependencies between tasks by matching file
# names. Then, file names can have "wildcards" in them, i.e.
# placeholders that will be made available to rule bodies
# (marked with the `run` block) as parameters.
#
# The execution therefore proceeds lazily. If we run, on 
# the command line
#
#     snakemake --cores all plot_syntetic_difficulty
#
# then snakemake will look at any missing input of the
# plot_syntetic_difficulty rule, and compute it, possibly
# computing any other missing file recursively, down to the
# input files.
#
# Rules in Snakemake (rather, their `run` block) can be
# written directly in Python

############################################################
#
# General setup
#
############################################################

# Configuration of the data directory. Setting the environment variable
# WORKGEN_DATA_DIR allows to specify a directory where to look
# for files and where to save preprocessed ones. The default
# is a hidden directory in the current working directory.
import os
DATA_DIR = os.environ.get("WORKGEN_DATA_DIR", ".data") # on nefeli: export WORKGEN_DATA_DIR=/mnt/hddhelp/workgen_data/
GENERATED_DIR = os.path.join(DATA_DIR, "generated")
RES_DIR = os.environ.get("WORKGEN_RES_DIR", "results") # on nefeli: export WORKGEN_RES_DIR=/mnt/hddhelp/workgen_results/
MESSI_DIR = os.environ.get("MESSI_INSTALL_DIR", "/home/qwang/projects/isax-modularized/build/")
MESSI_DIR_B = os.environ.get("MESSI_INSTALL_DIR", "/mnt/hddhelp/ts_benchmarks/messi/")
DSTREE_DIR = os.environ.get("DSTREE_INSTALL_DIR", "/mnt/hddhelp/ts_benchmarks/dstree/ds-tree-c/")

print("Results directory", RES_DIR)

# Here we list the minimum accepted snakemake version
from snakemake.utils import min_version
min_version("8.4")

# We can import other packages. One cool thing of Snakemake
# is that the entire Snakefile can be scripted with Python
import pandas as pd
from parameters import workloads

# All the workloads considered in our experiments. 
# See the `parameters` module.
WORKLOADS=workloads()

configfile: "workflow/config.yaml"



rule all:
    input:
        os.path.join(RES_DIR, "plots", "all-metrics.png"),
        os.path.join(RES_DIR, "plots", "index-performance.png"),
        os.path.join(RES_DIR, "plots", "correlations-bars.png"),
        os.path.join(RES_DIR, "plots", "sota-distcomps.png"),
        os.path.join(RES_DIR, "metrics", "index-performance.csv")

######################################################################
#
# Workload generation
#
######################################################################

# Here we generate workloads sets
rule generate_workload:
    input:
        os.path.join(DATA_DIR, "{dataset}.bin")
    output:
        workload = os.path.join(GENERATED_DIR,
          f"{WORKLOADS.wildcard_pattern}.bin"
        ),
        ground = os.path.join(GENERATED_DIR,
          f"{WORKLOADS.wildcard_pattern}/ground.npz"
        ),
    threads: workflow.cores
    run:
        import workflow.parameters as wparams
        import generate
        import logging
        import sys
        import os
        import read_data
        import utils
        #logging.basicConfig(level=logging.INFO)
        logging.basicConfig(level=logging.DEBUG)

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        workload_type = workload_config["workload_type"]
        if workload_type == "synthetic-simulated-annealing":
            generate.generate_workload_annealing(
              input[0],
              output["workload"],
              k = int(workload_config['k']),
              metric = workload_config["difficulty"],
              target_class = workload_config['target_class'],
              num_queries = int(workload_config['num_queries']),
              threads = threads,
              max_rounds = "return-any"
            )
        elif workload_type == "synthetic-sgd":
            generate.generate_workload_sgd(
              input[0],
              output[0],
              k = int(workload_config['k']),
              target_class = workload_config['target_class'],
              num_queries = int(workload_config['num_queries']),
              threads = threads,
            )
        elif workload_type == "synthetic-gaussian-noise":
            print("generating with gaussian noise", workload_config)
            generate.generate_workload_gaussian_noise(
                input[0],
                output["workload"],
                num_queries=workload_config["num_queries"],
                scale=workload_config["scale"],
            )
        elif workload_type == "file-based":
            print("linking file-based workload")
            os.symlink(
              os.path.abspath(
                os.path.join(DATA_DIR, workload_config["queries_filename"]) + ".bin"
              ),
              output["workload"]
            )
        elif workload_type == "empirical-difficulty-based":
            print("generating targeting empirical difficulty", workload_config)
            generate.generate_workload_empirical_difficulty(
                input[0],
                output["workload"],
                k=int(workload_config["k"]),
                empirical_difficulty_range=(workload_config["target_low"], workload_config["target_high"]),
                learning_rate=1.0,
                index_name=workload_config["index"],
                num_queries=workload_config["num_queries"],
                max_steps=1000,
                threads = threads,
            )
        else:
            raise ValueError(f"unknown workload type {workload_type}")

        print("Saving the ground truth")
        dataset, distance_metric = read_data.read_multiformat(input[0], "train")
        queryset, _ = read_data.read_multiformat(output["workload"], "test")
        utils.save_ground_truth(dataset, queryset, distance_metric, output["ground"])


# And finally here we compute the metrics of the synthetic
# query sets
rule eval_metrics:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        queries =
          os.path.join(GENERATED_DIR, 
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
    output:
        os.path.join(RES_DIR, f"metrics/{WORKLOADS.wildcard_pattern}.csv"),
    threads: workflow.cores
    run:
        import workflow.parameters as wparams
        import metrics

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        workload_descr = WORKLOADS.description_for(wildcards["workload_key"])

        metrics.metrics_csv(
          input['data'],
          input['queries'],
          output[0],
          workload_config['k'],
          additional_header = ["dataset", "workload_description"],
          additional_row = [workload_config['dataset'], workload_descr]
        )


# Here we bring in a single csv file the metrics computed
# on the syntehtic query sets
rule eval_metrics_concat:
    input:
        metrics = expand(
          os.path.join(RES_DIR, "metrics", "{params}.csv"),
          params=WORKLOADS.instance_patterns
        )
    output:
        #"results/metrics/all-metrics.csv"
        os.path.join(RES_DIR, "metrics","all-metrics.csv")
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input["metrics"]))
        df.to_csv(output[0], index=False)

# This rule plots the number of distance computations against
# the RC metric for different datasets, coloring points
# according to their "difficulty group"
rule plot_difficulty:
    input:
        #metrics = "results/metrics/all-metrics.csv"
        metrics = os.path.join(RES_DIR, "metrics","all-metrics.csv")
    output:
        #"results/plots/all-metrics.png"
        os.path.join(RES_DIR, "plots","all-metrics.png")
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input["metrics"])
        print(df)
        averages = df.groupby(["dataset", "workload_description"]).mean().reset_index()

        (
          so.Plot(df,
                  y="distcomp",
                  x="workload_description",
                  color="workload_description")
            .facet("dataset")
            .add(so.Dot())
            .save(output[0])
        )



rule convergence_plot:
    input:
        os.path.join(RES_DIR, "convergence", "all.csv")
    output:
        os.path.join("paper", "convergence.png")
    run:
        import pandas as pd
        import seaborn.objects as so
        import seaborn as sns

        import matplotlib
        import matplotlib.pyplot as plt
        matplotlib.use("Agg")

        df = pd.read_csv(input[0])
        df["dataset"] = df["dataset"].str.replace(".*/", "", regex=True)
        df["dataset"] = df["dataset"].str.replace("-.*", "", regex=True)
        df["method"] = df["method"].str.replace("sgd", "Hephaestus-Gradient")
        df["method"] = df["method"].str.replace("annealing", "Hephaestus-Annealing")

        def panel_plot(data, **kwargs):
            for qidx in data["query_index"].unique():
                for method in data["method"].unique():
                    line_data = data[(data["query_index"] == qidx) & (data["method"] == method)]
                    plt.plot(line_data["elapsed_s"], line_data["rc"], alpha=0.2, **kwargs)
            # now overlay the averages
            agg = data.groupby(["dataset", "method", "iteration"])[["rc", "elapsed_s"]].mean()
            plt.plot(agg["elapsed_s"], agg["rc"], linewidth=2, **kwargs)

        g = sns.FacetGrid(
            df,
            col="dataset",
            col_wrap=4,
            hue="method",
            sharex=False,
            sharey=False,
            aspect=1.5,
            height=2
        )
        g.set_xlabels("Elapsed time (s)")
        g.set_ylabels("Relative contrast")
        g.map_dataframe(panel_plot)
        g.add_legend()
        g.tight_layout()
        g.savefig(output[0])


rule convergence:
    input:
        expand(os.path.join(RES_DIR, "convergence", "{method}", "{dataset}.csv"), 
               method=["sgd", "annealing"],
               dataset=[
                  "fashion_mnist-euclidean-784-60K",
                  "glove-angular-104-1183514",
                  "nytimes-angular-256-289761",
                  "sald-128-100m",
                  "astro-256-100m",
                  "deep1b-96-100m",
                  "seismic-256-100m",
                  "rw-256-100m",
               ])
    output:
        os.path.join(RES_DIR, "convergence", "all.csv")
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


rule convergence_one:
    input:
        os.path.join(DATA_DIR, "{dataset}.bin")
    output:
        os.path.join(RES_DIR, "convergence", "{method}", "{dataset}.csv")
    run:
        import generate
        import logging

        num_queries = 10
        k = 10

        logging.basicConfig(level=logging.DEBUG)

        if wildcards["method"] == "sgd":
            generate.sgd_measure_convergence(
                input[0],
                output[0],
                k=k,
                num_queries=num_queries,
                learning_rate=10,
                max_steps=200
            )
        else:
            generate.annealing_measure_convergence(
                input[0],
                output[0],
                k=k,
                num_queries=num_queries,
                max_steps=1000
            )


rule convergence_scale_stats:
    input:
        os.path.join(RES_DIR, "scales", "convergence", "all_scales.csv")
    output:
        os.path.join("paper", "avg_iteration_times.png"),
        os.path.join("paper", "times.tex")
    run:
        import pandas as pd
        import seaborn as sns
        import matplotlib
        import matplotlib.pyplot as plt
        matplotlib.use("Agg")

        def compute_iteration_times(group):
            col = group["elapsed_s"] - group["elapsed_s"].shift().fillna(0)
            col.name = "iteration_time_s"
            return col

        df = pd.read_csv(input[0])
        df["dataset"] = df["dataset"].str.replace(".*/", "", regex=True)
        df["dataset"] = df["dataset"].str.replace("-.*", "", regex=True)
        df["scale"] = df["scale"].astype(str).str.replace("_", "", regex=False)
        df["scale"] = pd.Categorical(
            df["scale"],
            categories=["5m", "10m", "15m"],
            ordered=True
        )

        iteration_times = df.groupby([
            "dataset", "scale", "method", "query_index"
        ]).apply(compute_iteration_times).reset_index()
        print("unique datasets", df[[ "dataset", "scale" ]])

        avg_iteration_times = iteration_times.groupby([
            "dataset", "scale", "method"
        ])["iteration_time_s"].mean().reset_index()

        exec_times = df.groupby([
            "dataset", "scale", "method", "query_index"
        ])["elapsed_s"].max().reset_index().groupby([
            "dataset", "scale", "method"
        ])["elapsed_s"].mean().reset_index()

        time_info = pd.merge(exec_times, avg_iteration_times)

        sns.barplot(
            data = avg_iteration_times,
            x = "scale",
            y = "iteration_time_s",
            hue = "method"
        )
        plt.savefig(output[0])

        summary = time_info.pivot(columns="method", values=["iteration_time_s", "elapsed_s"], index=[ "dataset", "scale" ])
        summary.to_latex(output[1], float_format="%.1f")
        
        
    


rule convergence_scale_plot:
    input:
        os.path.join(RES_DIR, "scales", "convergence", "all_scales.csv")
    output:
        os.path.join("paper", "convergence_scales.png")
    run:
        import pandas as pd
        import seaborn.objects as so
        import seaborn as sns

        import matplotlib
        import matplotlib.pyplot as plt
        matplotlib.use("Agg")

        df = pd.read_csv(input[0])
        df["dataset"] = df["dataset"].str.replace(".*/", "", regex=True)
        df["dataset"] = df["dataset"].str.replace("-.*", "", regex=True)
        df["dataset_scale"] = df["dataset"] + df["scale"]

        def panel_plot(data, **kwargs):
            for qidx in data["query_index"].unique():
                for method in data["method"].unique():   
                    line_data = data[(data["query_index"] == qidx) & (data["method"] == method)]
                    plt.plot(line_data["elapsed_s"], line_data["rc"], alpha=0.2, **kwargs)
            # now overlay the averages
            agg = data.groupby(["dataset_scale", "method", "iteration"])[["rc", "elapsed_s"]].mean()
            plt.plot(agg["elapsed_s"], agg["rc"], linewidth=2, **kwargs)

        g = sns.FacetGrid(
            df,
            col="dataset_scale",
            col_wrap=4,
            hue="method",
            sharex=False,
            sharey=False,
            aspect=1.5,
            height=2
        )
        g.set_xlabels("Elapsed time (s)")
        g.set_ylabels("Relative contrast")
        g.map_dataframe(panel_plot)
        g.add_legend()
        g.tight_layout()
        g.savefig(output[0])


rule convergence_scale:
    input:
        expand(os.path.join(RES_DIR, "scales","convergence","{data_limit}","{method}","{dataset}.csv"), 
               method=["sgd", "annealing"],
               data_limit=[5000000,10000000, 15000000],
               dataset=[
                  "sald-128-100m",
                  "astro-256-100m",
                  "deep1b-96-100m",
                  "seismic-256-100m",
                  # "rw-256-100m",
               ])
    output:
        os.path.join(RES_DIR, "scales", "convergence", "all_scales.csv")
    run:
        import pandas as pd
        dfs = []
        for f in input: 
            data_limit = int(re.findall("scales/convergence/(\d+)", f)[0])
            df = pd.read_csv(f) 
            df["scale"] = f'_{int(data_limit/1000000)}m' 
            dfs.append(df) 
        pd.concat(dfs).to_csv(output[0]) 


rule convergence_one_scale:
    input:
        os.path.join(DATA_DIR, "{dataset}.bin")
    output:
        os.path.join(RES_DIR,"scales","convergence","{data_limit}","{method}","{dataset}.csv")
    run:
        import generate
        import logging

        num_queries = 10
        k = 10
        target_class = "hard"

        logging.basicConfig(level=logging.DEBUG)

        if wildcards["method"] == "sgd":
            generate.sgd_measure_convergence(
                input[0],
                output[0],
                k=k,
                num_queries=num_queries,
                learning_rate=10,
                max_steps=200,
                data_limit=int(wildcards["data_limit"]),
                target_class=target_class
            )
        else:
            generate.annealing_measure_convergence(
                input[0],
                output[0],
                k=k,
                num_queries=num_queries,
                max_steps=1000,
                data_limit=int(wildcards["data_limit"]),
                target_class=target_class
            )        


######################################################################
#
# Indices
#
######################################################################

rule plot_index_performance:
    input:
        metrics = os.path.join(RES_DIR, "metrics","index-performance.csv")
    output:
        os.path.join(RES_DIR, "plots","index-performance.png")
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        def get_family(c):
            if c.startswith("Gaussian"):
                return "GaussianNoise"
            if c.startswith("Annealing(faiss_ivf"):
                return "Annealing(faiss_ivf)"
            if c.startswith("Annealing(rc"):
                return "Annealing(rc)"
            return c

        df = pd.read_csv(input["metrics"])
        df.replace(
            {
                "GaussianNoise(0.1)": "GaussianNoise(easy)",
                "GaussianNoise(1.0)": "GaussianNoise(medium)",
                "GaussianNoise(10.0)": "GaussianNoise(difficult)",
                "Annealing(rc, 2.10, 1.90)": "Annealing(rc, easy)",
                "Annealing(rc, 1.80, 1.70)": "Annealing(rc, medium)",
                "Annealing(rc, 1.50, 1.30)": "Annealing(rc, difficult)",
                "Annealing(rc, 1.05, 1.03)": "Annealing(rc, medium)",
                "Annealing(rc, 1.01, 1.00)": "Annealing(rc, difficult)",
                "Annealing(faiss_ivf, 0.04, 0.06)": "Annealing(faiss_ivf, easy)",
                "Annealing(faiss_ivf, 0.19, 0.21)": "Annealing(faiss_ivf, medium)",
            },
            inplace=True
        )
        df.replace("File.*", "File", regex=True, inplace=True)
        df["family"] = df["workload"].map(get_family)
        # select the fastest configuration
        #df = df.iloc[df.groupby(["dataset", "workload", "query_index", "index_name", "target_recall"])["distcomp"].idxmin()]
        (
          so.Plot(df,
                  x="distcomp",
                  y="workload",
                  color="family")
            .facet("dataset", "index_name")
            .share(x=False)
            .add(so.Dot(), so.Jitter(0.2))
            .add(so.Dot(marker="*", pointsize=10, color="black"), so.Agg())
            .layout(size=(25, 10))
            .save(output[0])
        )

rule index_performance:
    input:
        [csv for csv in expand(
            os.path.join(RES_DIR, "metrics/{index}/{pattern}/results.csv"),
            index = ["messi", "faiss_hnsw", "faiss_ivf", "dstree"],
            pattern = WORKLOADS.instance_patterns
        ) if "index" not in WORKLOADS.config_from_path(csv)
          or WORKLOADS.config_from_path(csv)["index"] in csv
        ]
    output:
        os.path.join(RES_DIR, "metrics", "index-performance.csv")
    run:
        import pandas as pd
        dfs = []
        print("inputs are", input)
        for csv in input:
            k = WORKLOADS.config_from_path(csv)["k"]
            df = pd.read_csv(csv)
            df["k"] = k
            dfs.append(df)
        pd.concat(dfs).to_csv(output[0])

# rule run_messi_shell:
#     input:
#         data = os.path.join(DATA_DIR, "{dataset}.bin"),
#         workload =
#           os.path.join(GENERATED_DIR,
#             f"{WORKLOADS.wildcard_pattern}.bin"
#           ),

#     output:
#         os.path.join(
#           RES_DIR, 
#           f"metrics/messi/{WORKLOADS.wildcard_pattern}/results.csv"
#         ),
#     threads: workflow.cores
#     log:
#          os.path.join(RES_DIR,f"logs/messi/{WORKLOADS.wildcard_pattern}.log")
#     run:
#         import read_data as rd
#         workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
#         k = workload_config["k"]
#         _, data_samples, slength, _ = rd.parse_filename(wildcards["dataset"])
#         _, q_samples, q_slength, _ = rd.parse_filename(wildcards["workload_file"])
#         assert slength == q_slength
#         data_samples = min(data_samples, rd.MAX_DATA_LEN)
#         index = os.path.join(RES_DIR, "index/messi/", wildcards["dataset"])
#         print("Loading the first", data_samples, "vectors for MESSI")

#         # how much cheaper is a SAX distance computation wrt a full distance
#         # computation. We use this to aggregate the two types of distances into
#         # a single one
#         sax_rescale = config["sax_length"] / slength

#         # FIXME: how dow we get the running time per query?

#         try:
#             os.remove(log[0])
#         except FileNotFoundError:
#             pass

#         shell(MESSI_DIR + "/isax \
# --database_filepath='{input.data}' \
# --query_filepath='{input.workload}' \
# --database_size={data_samples} \
# --query_size={q_samples} \
# --sax_length={config[sax_length]} \
# --sax_cardinality={config[sax_card]} \
# --cpu_cores={config[cpu_cores]} \
# --log_filepath='{log}' \
# --series_length={slength} \
# --adhoc_breakpoints \
# --leaf_size={config[leaf_size]} \
# --k={k} \
# --exact_search \
# --with_id ")

# ## new messi shell
# ## ./bin/MESSI --dataset /data/qwang/datasets/sald-128-100m.bin  --initial-lbl-size 2000 --leaf-size 2000 --min-leaf-size 2000 --function-type 3 --cpu-type 82 --dataset-size 5000000 --flush-limit 300000 --read-block 20000 --queries /data/qwang/datasets/sald-128-1k.bin --queries-size 10 --queue-number 2 --timeseries-size 128 --in-memory --topk --k-size 10

# #         shell(MESSI_DIR_B + "bin/MESSI \
# # --dataset '{input.data}' \
# # --initial-lbl-size 2000 \
# # --leaf-size {config[leaf_size]} \
# # --min-leaf-size 2000 \
# # --queries='{input.workload}' \
# # --dataset-size {data_samples} \
# # --queries-size {q_samples} \
# # --timeseries-size={slength} \
# # --topk \
# # --k-size {k} \
# # --function-type 3 \
# # --cpu-type 82 \
# # --flush-limit 300000 \
# # --read-block 20000 \
# # --queue-number 2 \
# # --in-memory \
# # > '{log}' ")

#         res = []
#         for line_idx, log_line in enumerate(shell("grep -F 'l2square' {log} | grep 'query_engine.c' | cut -d ' ' -f 6,8,11", read=True, iterable=True)):
#             q_idx, dists, sax_dist = list(map(int, log_line.split()))
#             # distcomp = dists + sax_rescale * sax_dist
#             distcomp = dists
#             distcomp = distcomp / data_samples

#             alg_name = "messi"
#             recall = 1.0

#             if q_idx >= q_samples:
#                 alg_name = "messi_apprx"
#                 q_idx -= q_samples
#                 recall = None

#             res.append({
#                 "dataset": wildcards["dataset"],
#                 "workload": WORKLOADS.description_for(wildcards["workload_key"]),
#                 "query_index": q_idx,
#                 "index_name": alg_name,
#                 "index_params": "",
#                 "time_s": None,
#                 "distcomp": distcomp,
#                 "recall": recall,
#             })
#         try:
#             os.remove(output[0])
#         except FileNotFoundError:
#             pass

#         df = pd.DataFrame(res)
#         df.to_csv(output[0], index=False)

## messi version of Botao ##
rule run_messi_shell:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),

    output:
        os.path.join(
          RES_DIR, 
          f"metrics/messi/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    log:
         os.path.join(RES_DIR,f"logs/messi/{WORKLOADS.wildcard_pattern}.log")
    run:
        import read_data as rd
        import numpy as np
        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        _, data_samples, slength, _ = rd.parse_filename(wildcards["dataset"])
        _, q_samples, q_slength, _ = rd.parse_filename(wildcards["workload_file"])
        assert slength == q_slength
        data_samples = min(data_samples, rd.MAX_DATA_LEN)
        index = os.path.join(RES_DIR, "index/messi/", wildcards["dataset"])

        # # check that data and queries are normalized
        # data, _ = rd.read_multiformat(input["data"], "train")
        # assert np.all(np.isclose(np.linalg.norm(data, axis=1), 1))
        # queries, _ = rd.read_multiformat(input["workload"], "test")
        # assert np.all(np.isclose(np.linalg.norm(queries, axis=1), 1))
        # data = None
        # queries = None
        
        leaf_size = 2000
        min_leaf_size = 2000
        initial_lbl_size = 2000

        print("Loading the first", data_samples, "vectors for MESSI")

        # how much cheaper is a SAX distance computation wrt a full distance
        # computation. We use this to aggregate the two types of distances into
        # a single one
        sax_rescale = config["sax_length"] / slength

        # FIXME: how dow we get the running time per query?

        try:
            os.remove(log[0])
        except FileNotFoundError:
            pass



        if (wildcards["dataset"]=="fashion_mnist-euclidean-784-60K"):
            leaf_size = 60000
            min_leaf_size = 60000
            initial_lbl_size = 60000





# shell example
# ./bin/MESSI --dataset /data/qwang/datasets/sald-128-100m.bin  --initial-lbl-size 2000 --leaf-size 2000 --min-leaf-size 2000 --function-type 3 --cpu-type 82 --dataset-size 5000000 --flush-limit 300000 --read-block 20000 --queries /data/qwang/datasets/sald-128-1k.bin --queries-size 10 --queue-number 2 --timeseries-size 128 --in-memory --topk --k-size 10



        shell(MESSI_DIR_B + "bin/MESSI \
--dataset '{input.data}' \
--initial-lbl-size {initial_lbl_size} \
--leaf-size {leaf_size} \
--min-leaf-size {min_leaf_size} \
--queries='{input.workload}' \
--dataset-size {data_samples} \
--queries-size {q_samples} \
--timeseries-size {slength} \
--sax-cardinality {config[sax_card]} \
--topk \
--k-size {k} \
--function-type 3 \
--cpu-type 82 \
--flush-limit 300000 \
--read-block 20000 \
--queue-number 2 \
--in-memory \
> '{log}' ")

# grep -F 'query' ../tmp/messi_test.log | cut -f 2,4

        res = []
        for line_idx, log_line in enumerate(shell("grep -F 'query' {log} |  cut -f 2,4", read=True, iterable=True)):
            q_idx, dists = list(map(int, log_line.split()))
            # distcomp = dists + sax_rescale * sax_dist
            distcomp = dists
            distcomp = distcomp / data_samples 

            alg_name = "messi"
            recall = 1.0

            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": alg_name,
                "index_params": "",
                "time_s": None,
                "distcomp": distcomp,
                "recall": recall,
            })
        try:
            os.remove(output[0])
        except FileNotFoundError:
            pass

        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)

rule run_dstree_shell:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),

    output:
        os.path.join(
          RES_DIR, 
          f"metrics/dstree/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    log:
         os.path.join(RES_DIR,f"logs/dstree/{WORKLOADS.wildcard_pattern}.log")
    run:
        import read_data as rd
        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        _, data_samples, slength, _ = rd.parse_filename(wildcards["dataset"])
        _, q_samples, q_slength, _ = rd.parse_filename(wildcards["workload_file"])
        assert slength == q_slength
        data_samples = min(data_samples, rd.MAX_DATA_LEN)
        index = os.path.join(RES_DIR, "index/dstree/", wildcards["dataset"])
        print("Loading the first", data_samples, "vectors for DS-TREE")

        try:
            os.remove(log[0])
        except FileNotFoundError:
            pass

##- -  just index - -
## bin/dstree --dataset /mnt/hddhelp/workgen_data/sald-128-100m.bin --dataset-size 5000000 --buffer-size 19531 --leaf-size 1000 --index-path /mnt/hddhelp/ts_benchmarks/dstree/out/  --ascii-input 0 --mode 0 --timeseries-size 128

## - - just query â€” -
## bin/dstree --dataset /mnt/hddhelp/workgen_data/sald-128-100m.bin --dataset-size 5000000  --queries /mnt/hddhelp/workgen_data/sald-128-1k.bin  --queries-size 100 --buffer-size 19531 --leaf-size 1000 --index-path /mnt/hddhelp/ts_benchmarks/dstree/out/  --ascii-input 0 --mode 1 --timeseries-size 128 --k 1 --epsilon 0 --delta 1
        if not os.path.exists(index):
            shell(DSTREE_DIR + "bin/dstree \
                --dataset '{input.data}' \
                --dataset-size {data_samples} \
                --buffer-size 19531 \
                --leaf-size 1000 \
                --index-path '{index}' \
                --ascii-input 0 \
                --mode 0 \
                --timeseries-size {slength} ")

        shell(DSTREE_DIR + "bin/dstree \
            --dataset '{input.data}' \
            --dataset-size {data_samples} \
            --queries '{input.workload}' \
            --queries-size {q_samples} \
            --buffer-size 19531 \
            --leaf-size 1000 \
            --index-path '{index}' \
            --ascii-input 0 \
            --mode 1 \
            --timeseries-size {slength} \
            --k 1 \
            --epsilon 0 \
            --delta 1 \
            > '{log}'")


# grep -F 'Query_total_loaded_ts_count' {log}  | grep '1$' | cut  -f 2,4

        res = []
        for line_idx, log_line in enumerate(shell("grep -F 'Query_total_loaded_ts_count' {log}  | grep '1$' | cut  -f 2,4", read=True, iterable=True)):
            dists, q_idx = list(map(int, log_line.split()))
            # distcomp = dists + sax_rescale * sax_dist
            distcomp = dists 
            distcomp = distcomp / data_samples 

            alg_name = "dstree"
            recall = 1.0

            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": alg_name,
                "index_params": "",
                "time_s": None,
                "distcomp": distcomp,
                "recall": recall,
            })
        try:
            os.remove(output[0])
        except FileNotFoundError:
            pass

        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)

rule run_faiss_hnsw:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
        ground = os.path.join(GENERATED_DIR,
          f"{WORKLOADS.wildcard_pattern}/ground.npz"
        ),
    output:
        os.path.join(
          RES_DIR, 
          f"metrics/faiss_hnsw/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    run:
        import numpy as np
        import read_data
        import faiss
        import utils
        import time
        import pandas as pd
        import json
        import indices
        from indices import FAISS_LOCK
        from metrics import EmpiricalDifficultyHNSW

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        dataset, distance_metric = read_data.read_multiformat(input["data"], "train")
        queryset, _ = read_data.read_multiformat(input["workload"], "test")
        ground_truth = np.load(input.ground)["distances"]

        print("Populating index")
        exact_index = faiss.IndexFlatL2(dataset.shape[1])
        exact_index.add(dataset)
        target_recall = 0.95
        evaluator= EmpiricalDifficultyHNSW(dataset, target_recall, exact_index, distance_metric)
        print("done")

        res = []

        for q_idx in range(queryset.shape[0]):
            print("running query", q_idx)
            q = queryset[q_idx,:].reshape(1, -1)
            ground_distances = ground_truth[q_idx, :] 
            distcomp, rec = evaluator.evaluate(q, k, ground_distances, return_recall=True)
            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": "faiss_hnsw",
                "target_recall": target_recall,
                "index_params": json.dumps({
                    "index_params": evaluator.index_params,
                    "efSearch": evaluator.index.hnsw.efSearch
                }, sort_keys=True),
                "distcomp": distcomp,
                "recall": rec,
            })
        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)


rule run_faiss_ivf:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),
        workload =
          os.path.join(GENERATED_DIR,
            f"{WORKLOADS.wildcard_pattern}.bin"
          ),
        ground = os.path.join(GENERATED_DIR,
          f"{WORKLOADS.wildcard_pattern}/ground.npz"
        ),
    output:
        os.path.join(
          RES_DIR, 
          f"metrics/faiss_ivf/{WORKLOADS.wildcard_pattern}/results.csv"
        ),
    threads: workflow.cores
    run:
        import numpy as np
        import read_data
        import faiss
        import utils
        import time
        import pandas as pd
        import json
        import indices
        from indices import FAISS_LOCK
        from metrics import EmpiricalDifficultyIVF

        workload_config = WORKLOADS.config_for( wildcards["workload_key"] )
        k = workload_config["k"]
        dataset, distance_metric = read_data.read_multiformat(input["data"], "train")
        queryset, _ = read_data.read_multiformat(input["workload"], "test")
        ground_truth = np.load(input.ground)["distances"]

        print("Populating index")
        exact_index = faiss.IndexFlatL2(dataset.shape[1])
        exact_index.add(dataset)
        target_recall = 0.95
        evaluator= EmpiricalDifficultyIVF(dataset, target_recall, exact_index, distance_metric)
        print("done")

        res = []

        for q_idx in range(queryset.shape[0]):
            print("running query", q_idx)
            q = queryset[q_idx,:].reshape(1, -1)
            ground_distances = ground_truth[q_idx,:] # utils.compute_distances(q, None, distance_metric, dataset)[0, :]
            distcomp, rec = evaluator.evaluate(q, k, ground_distances, return_recall=True)
            res.append({
                "dataset": wildcards["dataset"],
                "workload": WORKLOADS.description_for(wildcards["workload_key"]),
                "query_index": q_idx,
                "index_name": "faiss_ivf",
                "target_recall": target_recall,
                "index_params": json.dumps({
                    "index_params": evaluator.index_params,
                    "nprobe": evaluator.index.nprobe
                }, sort_keys=True),
                "distcomp": distcomp,
                "recall": rec,
            })
        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)


############################################################
#
# Correlation assessment
#
############################################################

rule correlation_analysis:
    input:
        os.path.join(RES_DIR, "metrics", "all-metrics.csv"),
    output:
        scatter = os.path.join(RES_DIR, "plots", "correlations.png"),
        correlations_bars = os.path.join(RES_DIR, "plots", "correlations-bars.png"),
        table = os.path.join(RES_DIR, "metrics", "correlations.csv"),
    run:
        import pandas as pd
        import numpy as np
        import seaborn.objects as so
        from scipy.stats import spearmanr, kendalltau

        def ts_fun(ys):
            for c in ys.columns:
                ys[c], bins = pd.qcut(ys[c], 100, labels=False, duplicates="drop", retbins=True)
            return ys.reset_index(drop=True)

        metrics = pd.read_csv(input[0])
        #metrics = metrics[metrics["workload_description"].str.startswith("File")]
        metrics.set_index("dataset", inplace=True)
        metrics = metrics[metrics.columns[metrics.dtypes == np.float64]]
        print(metrics)

        correlations = metrics.groupby(metrics.index).corr("kendall")
        correlations = correlations[["distcomp"]]
        correlations.reset_index(inplace=True)
        correlations.rename({"level_1": "measure", "distcomp": "distcomp_correlation"}, axis=1, inplace=True)
        correlations = correlations[correlations["measure"] != "distcomp"]
        correlations.to_csv(output["table"])
        correlations["sign"] = correlations["distcomp_correlation"] > 0
        correlations["distcomp_correlation"] = np.abs(correlations["distcomp_correlation"])

        df = metrics.reset_index().melt(
            value_vars=["lid_10", "rc_10", "exp_20|10", "eps_0.1"], 
            id_vars=["distcomp", "dataset"]
        )
        
        (
          so.Plot(df,
                  y="distcomp",
                  x="value")
            .facet("variable", "dataset")
            .share(x=False)
            .scale(x="log", y="log")
            .add(so.Dots(pointsize=1))
            .layout(size=(16, 16))
            .save(output["scatter"])
        )

        (
          so.Plot(correlations,
                  x="distcomp_correlation",
                  y="measure",
                  color="sign")
            .facet(row="dataset")
            .add(so.Bars())
            .layout(size=(8, 12))
            .save(output["correlations_bars"])
        )

############################################################
#
# Data preparation
#
############################################################


rule prepare_data_file:
    output:
        os.path.join(DATA_DIR, "{name}.hdf5")
    wildcard_constraints:
        name="[a-zA-Z0-9-]+"
    run:
        import read_data
        name = wildcards["name"]
        read_data.download(f"https://ann-benchmarks.com/{name}.hdf5", output[0])


rule convert_annbench_to_bin:
    output:
        expand(
            os.path.join(DATA_DIR, "{name}.bin"),
            name=[
                "fashion_mnist-euclidean-784-60K",
                "glove-angular-32-1183514",
                "glove-angular-104-1183514",
                "nytimes-angular-256-289761",
            ]
        )
    run:
        import read_data
        features_map = {32: 25, 104: 100}
        for data_output in output:
            print("data output", data_output)
            name, samples, features, distance_metric = read_data.parse_filename(data_output)
            aname = name.replace("_", "-")
            ann_feats = features_map.get(features, features)
            ann_bench_name = f"{aname}-{ann_feats}-{distance_metric}"
            tmp = os.path.join(DATA_DIR, f"{ann_bench_name}.hdf5")
            read_data.download(f"https://ann-benchmarks.com/{ann_bench_name}.hdf5", tmp)
            read_data.hdf5_to_bin(tmp, data_output, "train")

            qsamples = read_data.read_multiformat(tmp, "test")[0].shape[0]
            queries_output = os.path.join(
                os.path.dirname(data_output),
                f"queries_{name}-{distance_metric}-{features}-{qsamples}.bin"
            )
            print("queries output", queries_output)
            read_data.hdf5_to_bin(tmp, queries_output, "test")


######################################################################
#
# Paper content
#
######################################################################

# Produce things that will go in the paper, starting from the 
# results_nefeli directory
rule paper_all:
    input:
        "paper/index-table-k10.tex",
        "paper/index-table-k1.tex",
        "paper/index-k10.png",
        "paper/index-k1.png",
        "paper/actual_relative_contrasts_k10.png",
        "paper/actual_relative_contrasts_k1.png",
        "paper/actual_lid_k10.png",
        "paper/actual_lid_k1.png",
        "paper/correlations_rc_10.png",
        "paper/correlations_exp_20|10.png",
        "paper/correlations_lid_10.png",
        "paper/correlations_eps_1.png",
        "paper/correlations_eps_0.5.png",
        "paper/correlations_eps_0.05.png",
        "paper/correlations-bars.png",


rule paper_index_table:
    input:
        "results_nefeli/metrics/index-performance.csv"
    output:
        "paper/index-table-k{k}.tex"
    run:
        import pandas as pd
        k = int( wildcards["k"] )
        perf = pd.read_csv(input[0])
        perf = perf[perf["k"] == k]
        perf.rename(columns={"index_name": "index"}, inplace=True)
        perf.replace("File.*", "File", regex=True, inplace=True)
        perf.replace("faiss_ivf", "FaissIVF", regex=True, inplace=True)
        perf.replace("faiss_hnsw", "FaissHNSW", regex=True, inplace=True)
        perf.replace("fashion_mnist", "fashion-mnist", regex=True, inplace=True)
        perf.replace({"dataset": "-.*"}, "", regex=True, inplace=True)
        perf["method"] = perf["workload"].str.replace("\\(.*", "", regex=True)
        perf["difficulty"] = perf["workload"].str.extract(r"(easy|medium|hard\+|hard)")
        perf["difficulty"].fillna("-", inplace=True)
        perf["method"] = perf["method"].str.replace("File", "Baseline")

        print(perf["workload"].unique())
        perf = perf[perf["difficulty"] != "hard+"]
        perf["difficulty"] = perf["difficulty"].str.replace("+", "")

        perf["difficulty"] = pd.Categorical(
            perf["difficulty"],
            categories=["-", "easy", "medium", "hard", "hard+"],
            ordered=True
        )
        perf["method"] = pd.Categorical(
            perf["method"],
            categories=["Baseline", "GaussianNoise", "Annealing", "SGD"],
            ordered=True
        )

        perf = perf[perf["index"] != "messi_apprx"]
        tab = perf.groupby(["index", "dataset", "method", "difficulty"])[["distcomp"]].mean()
        tab = tab.dropna(axis=0)
        tab = tab.reset_index(["method", "difficulty"]).pivot(columns=["method", "difficulty"], values="distcomp")
        tab.to_latex(output[0], float_format="%.5f", column_format="ll|r|rrr|rrr|rrr")
        #tab.to_latex(output[0], float_format="%.1e")


rule paper_index_plot:
    input: "results_nefeli/metrics/index-performance.csv"
    output: "paper/index-k{k}.png"
    script: "plots/paper_index_plot.py"


rule paper_actual_relative_contrasts:
    input:
        "results_nefeli/metrics/all-metrics.csv"
    output:
        "paper/actual_relative_contrasts_k{k}.png",
        "paper/actual_lid_k{k}.png"
    run:
        import pandas as pd
        import seaborn.objects as so
        
        k = int( wildcards["k"] )
        rc_col = f"rc_{k}"
        lid_col = f"lid_{k}"

        metrics = pd.read_csv(input[0])
        metrics = metrics[metrics[rc_col] < 100]
        metrics.replace("fashion_mnist", "fashion-mnist", regex=True, inplace=True)
        metrics.replace({"dataset": "-.*"}, "", regex=True, inplace=True)
        metrics["method"] = metrics["workload_description"].str.replace("\\(.*", "", regex=True)
        metrics["difficulty"] = metrics["workload_description"].str.extract(r"(easy|medium|hard\+|hard)")
        metrics["difficulty"].fillna("-", inplace=True)
        metrics["difficulty"] = pd.Categorical(
            metrics["difficulty"],
            categories=["-", "easy", "medium", "hard", "hard+"],
            ordered=True
        )
        metrics = metrics[metrics["method"] != "File"]
        metrics["method"] = pd.Categorical(
            metrics["method"],
            categories=["GaussianNoise", "Annealing", "SGD"],
            ordered=True
        )

        p = (
          so.Plot(metrics,
                  x=rc_col,
                  y="method",
                  color="difficulty")
            .layout(size=(14, 7))
            .facet("dataset", wrap=4)
            .share(x=False)
            .add(so.Dot(), so.Dodge())
            .label(color="Difficulty")
            .save(output[0])
        )
        p = (
          so.Plot(metrics,
                  x=lid_col,
                  y="method",
                  color="difficulty")
            .layout(size=(8, 16))
            .facet("dataset", wrap=2)
            .share(x=False)
            .add(so.Dot(), so.Dodge())
            .label(color="Difficulty")
            .save(output[1])
        )

rule paper_correlations_specific:
    input:
        "results_nefeli/metrics/all-metrics.csv"
    output:
        "paper/correlations_{metric}.png",
    run:
        import pandas as pd
        import numpy as np
        import seaborn as sns
        import matplotlib

        matplotlib.use("Agg")

        metric = wildcards["metric"]

        df = pd.read_csv(input[0])
        df["dataset"] = df["dataset"].str.replace("-.*", "", regex=True)
        df = df[df["dataset"] != "fashion_mnist"]

        # remove outliers
        #if metric in ["rc_10", "exp_20|10", "lid_10", "eps_1"]:
        q = df[metric].quantile(0.99)
        df = df[df[metric] < q]

        def quantizer(group, column="rc_10"):
            group = group[group[column] >= 0]
            nbins = 100
            try:
                group["binned_col"] = pd.qcut(group[column], 100)
            except:
                group["binned_col"] = pd.cut(group[column], 100)
            group = group.groupby("binned_col", observed=True)[[column, "distcomp"]].mean().dropna().reset_index(drop=True)
            return group


        def quantize_by(df, column="rc_10"):
            df = df.groupby("dataset").apply(quantizer, column, include_groups=False).reset_index()
            del df["level_1"]
            corrs = df.groupby("dataset").corr("kendall")["distcomp"].reset_index()
            corrs.columns = ["dataset", "metric", "correlation"]
            corrs = corrs[corrs["metric"] != "distcomp"]
            corrs["metric"] = corrs["metric"].replace()
            corrs.set_index("dataset", inplace=True)
            corrs = corrs["correlation"]
            return df, corrs

        df, corrs = quantize_by(df, metric)

        g = sns.FacetGrid(df, col="dataset", sharex=False, sharey=False)
        g.map_dataframe(
            sns.scatterplot,
            x = metric,
            y = "distcomp"
        )

        remapping = {
          "lid_10": "$LID_{10}$",
          "rc_10": "$RC_{10}$",
          "exp_20|10": "$Expansion_{20|10}$",
          "eps_0.05": r"$\alpha_{0.05, 10}$",
          "eps_0.1": r"$\alpha_{0.1, 10}$",
          "eps_0.5": r"$\alpha_{0.5, 10}$",
          "eps_1": r"$\alpha_{1, 10}$",
        }

        for (i, j, _), fdata in g.facet_data():
            dataset = fdata["dataset"].unique()[0]
            ax = g.facet_axis(i, j)
            xtext = 1 if metric in ["rc_10", "exp_20|10"] else 0
            ha = "right" if metric in ["rc_10", "exp_20|10"] else "left"
            ax.text(xtext, 1, "$\\tau$=%.3f" % (corrs.loc[dataset]),
                    size=15,
                    transform=ax.transAxes, 
                    ha=ha, 
                    va="top",
                    parse_math=True)

        g.set_axis_labels(x_var=remapping[metric],y_var="empirical hardness")
        g.set(yscale="log")
        g.set(xscale="log")
        g.tight_layout()
        g.savefig(output[0])


rule paper_correlations_bars:
    input:
        "results_nefeli/metrics/all-metrics.csv"
    output:
        "paper/correlations-bars.png",
    run:
        import pandas as pd
        import numpy as np
        import seaborn.objects as so
        import matplotlib
        import matplotlib.pyplot as plt

        matplotlib.use("Agg")

        plt.rcParams.update({
            "text.usetex": True,
            "font.family": "Helvetica"
        })

        df = pd.read_csv(input[0])
        df["dataset"] = df["dataset"].str.replace("-.*", "", regex=True)
        df = df[df["dataset"] != "fashion_mnist"]
        print(df)


        def quantizer(group, column="rc_10"):
            group = group[np.isfinite(group[column])].copy()
            try:
                group["binned_col"] = pd.qcut(group[column], 100)
            except:
                group["binned_col"] = pd.cut(group[column], 100)
            group = group.groupby("binned_col", observed=True)[[column, "distcomp"]].mean().dropna().reset_index(drop=True)
            return group


        def quantize_by(df, column="rc_10"):
            df = df.groupby("dataset").apply(quantizer, column, include_groups=False).reset_index()
            del df["level_1"]
            corrs = df.groupby("dataset").corr("kendall")["distcomp"].reset_index()
            corrs = corrs[corrs["level_1"] != "distcomp"]
            del corrs["level_1"]
            corrs.set_index("dataset", inplace=True)
            corrs.columns = [column]
            return df, corrs

        dfs = []
        for metric in ["lid_10", "rc_10", "exp_20|10"] + ["eps_" + str(e) for e in [ 0.05, 0.1, 0.5, 1 ]]:
            # remove outliers
            if metric in ["rc_10", "exp_20|10"]:
                q = df[metric].quantile(0.99)
                ldf = df[df[metric] < q]
            else:
                ldf = df.copy()
            _f, corrs = quantize_by(ldf, metric)
            dfs.append(corrs)

        corrs = dfs[0]
        for df in dfs[1:]:
            corrs = pd.merge(corrs, df, left_index=True, right_index=True)
        corrs = corrs.stack().reset_index()
        corrs.columns = ["dataset", "metric", "correlation"]
        corrs["sign"] = corrs["correlation"] > 0
        corrs["correlation"] = np.abs( corrs["correlation"] )
        corrs["metric"] = corrs["metric"].replace({
          "lid_10": "$LID_{10}$",
          "rc_10": "$RC_{10}$",
          "exp_20|10": "$Expansion_{20|10}$",
          "eps_0.05": r"$\alpha_{0.05, 10}$",
          "eps_0.1": r"$\alpha_{0.1, 10}$",
          "eps_0.5": r"$\alpha_{0.5, 10}$",
          "eps_1": r"$\alpha_{1, 10}$",
        })

        (
          so.Plot(corrs,
                  x="correlation",
                  y="metric",
                  color="sign")
            .facet(row="dataset")
            .add(so.Bars())
            .layout(size=(8, 12))
            .save(output[0])
        )

rule sota_distcomps:
    input:
        metrics = os.path.join("results_nefeli", "metrics","all-metrics.csv"),
        index_perf = os.path.join("results_nefeli", "metrics","index-performance.csv"),
    output:
        main = os.path.join("paper", "sota-distcomps.png"),
        legend = os.path.join("paper", "sota-distcomps-legend.png")
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd
        import numpy as np

        df = pd.read_csv(input["index_perf"])
        df = df[df["k"] == 10]
        df = df[df["workload"].str.startswith("File")]
        df = df[df["index_name"] != "messi_apprx"]
        df["dataset"] = df["dataset"].str.replace(".*/", "", regex=True)
        df["dataset"] = df["dataset"].str.replace("-.*", "", regex=True)
        ndatasets = len( df["dataset"].unique() )
        n_indices = len( df["index_name"].unique() )

        colors = ["#113238","#418D64","#E2E062"]

        def doplot(data, ax=None):
            if ax is None:
                ax = plt.gca()
            ptiles = np.percentile(data, [2.5, 10, 25, 75, 90, 97.5])
            median = np.median(data)
            print("median is", median, "percentiles", ptiles)
            sns.kdeplot(
                data,
                ax=ax,
                color="gray",
                edgecolor="gray",
                fill=True,
                cut=0,
                zorder=100
            )
            ylim = ax.get_ylim()
            print("ylim", ylim)

            #ax.hist(subset["distcomp"], bins=100, color="lightgray")
            miny, maxy = ( -0.5 * ylim[1] ), (-0.05 * ylim[1])
            #ax.scatter(median, ylim[1]/2, zorder=200, color="red")
            ax.axvline(median, zorder=200, color="red")
            ax.text(
                median + (1 if median < 0.5 else -1) * 0.02,
                ylim[1],
                "%.4f" % (median),
                zorder=200,
                color="black",
                ha = "left" if median < 0.5 else "right",
                va="top"
            )
            ax.fill_between([ptiles[0], ptiles[5]], miny, maxy, color=colors[2])
            ax.fill_between([ptiles[1], ptiles[4]], miny, maxy, color=colors[1])
            ax.fill_between([ptiles[2], ptiles[3]], miny, maxy, color=colors[0])
            ax.set_xlim((0, 1))


        fig, axs = plt.subplots(nrows=ndatasets, ncols=4, figsize=(10, 4))

        for i, dataset in enumerate( sorted( df["dataset"].unique() ) ):
            for j, index in enumerate(df["index_name"].unique()):
                ax = axs[i][j]
                subset = df[(df["dataset"] == dataset) & (df["index_name"] == index)]
                doplot(subset["distcomp"], ax=ax)

                ax.spines["top"].set_visible(False)
                ax.spines["right"].set_visible(False)
                if i == 0:
                    ax.set_title("index: {}".format(index))
                if i < ndatasets - 1:
                    ax.spines["bottom"].set_visible(False)
                    ax.get_xaxis().set_visible(False)
                ax.set_xlabel("")
                if j == 0:
                    ax.set_ylabel(dataset, rotation=0, ha="right")
                    ax.set_yticks([])
                else:
                    ax.get_yaxis().set_visible(False)
                ax.spines["left"].set_visible(False)


        axs[-1][1].set_xlabel("fraction of distance computations")

        plt.tight_layout(pad=0)
        plt.savefig(output["main"], dpi=300)

        # LEGEND
        plt.figure(figsize=(3, 2))

        datalegend = np.random.default_rng(1234).exponential(0.3, size=1000)
        datalegend = datalegend[(datalegend > 0) & (datalegend < 1)]

        doplot(datalegend)
        ax = plt.gca()
        ax.set_axis_off()

        yoff = -3
        ax.annotate(
            "50% of the\ndistance computations\nfall in this range", 
            (0.22, -1.25),
            (0.15, yoff),
            arrowprops=dict(
                #width=.5,
                color="k",
                arrowstyle="Simple, tail_width=0.01, head_width=.5, head_length=.5",
                connectionstyle="arc3,rad=.3"
            ),
            ha="center", 
            va="top"
        )
        ax.annotate(
            "80%",
            (0.5, -1.25),
            (0.5, yoff),
            arrowprops=dict(
                color="k",
                arrowstyle="Simple, tail_width=0.01, head_width=.5, head_length=.5",
                connectionstyle="arc3,rad=.3"
            ),
            ha="center", 
            va="top"
        )
        ax.annotate(
            "95%",
            (0.75, -1.25),
            (0.75, yoff),
            arrowprops=dict(
                color="k",
                arrowstyle="Simple, tail_width=0.01, head_width=.5, head_length=.5",
                connectionstyle="arc3,rad=.3"
            ),
            ha="center", 
            va="top"
        )
        ax.annotate(
            "median",
            (0.45, 2.3),
            (0.75, 2),
            arrowprops=dict(
                color="k",
                arrowstyle="Simple, tail_width=0.01, head_width=.5, head_length=.5",
                connectionstyle="arc3,rad=.3"
            ),
            ha="center", 
            va="top"
        )
        ax.text(0.0, 2.8, "Legend", color="gray", size=13)

        plt.tight_layout(pad=0)
        plt.savefig(output["legend"], dpi=300)
