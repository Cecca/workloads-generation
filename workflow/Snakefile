############################################################
#
# General introduction
# ====================
#
# This Snakefile coordinates the execution of the following, 
# among other things:
#
#  - Download and preprocessing of datasets
#  - Generation of synthetic query workloads under different
#    parameterizations
#  - Plotting of the resulting difficulty metrics
#
# This file works a lot like a Makefile with wildcard
# patterns. In particular, there are several `rule` blocks,
# each defining its input and output files. Snakemake then
# infers the dependencies between tasks by matching file
# names. Then, file names can have "wildcards" in them, i.e.
# placeholders that will be made available to rule bodies
# (marked with the `run` block) as parameters.
#
# The execution therefore proceeds lazily. If we run, on 
# the command line
#
#     snakemake --cores all plot_syntetic_difficulty
#
# then snakemake will look at any missing input of the
# plot_syntetic_difficulty rule, and compute it, possibly
# computing any other missing file recursively, down to the
# input files.
#
# Rules in Snakemake (rather, their `run` block) can be
# written directly in Python

############################################################
#
# General setup
#
############################################################

# Configuration of the data directory. Setting the environment variable
# WORKGEN_DATA_DIR allows to specify a directory where to look
# for files and where to save preprocessed ones. The default
# is a hidden directory in the current working directory.
import os
DATA_DIR = os.environ.get("WORKGEN_DATA_DIR", ".data") # on nefeli: export WORKGEN_DATA_DIR=/mnt/hddhelp/workgen_data/
GENERATED_DIR = os.path.join(DATA_DIR, "generated")
RES_DIR = os.environ.get("WORKGEN_RES_DIR", "results") # on nefeli: export WORKGEN_RES_DIR=/mnt/hddhelp/workgen_results/
MESSI_DIR = os.environ.get("MESSI_INSTALL_DIR", "/home/qwang/projects/isax-modularized/build/isax")

# Here we list the minimum accepted snakemake version
from snakemake.utils import min_version
min_version("7.24")

# We can import other packages. One cool thing of Snakemake
# is that the entire Snakefile can be scripted with Python
import pandas as pd
from parameters import setup_param_space, get_data_path, dataset_param_space, metrics_param_space, get_samples

# This `paramspace` variable is a wrapper around a dataframe
# that holds all the parameter combinations we want to
# test. These parameter combinations are set up in the
# function in setup_param_space().
# More on this here: https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#parameter-space-exploration
paramspace = setup_param_space()

# For convenience, here we extract the set of datasets we
# use in this evaluation.
DATASETS = set(paramspace.dataframe['dataset'])
BIN_CONFIGS = pd.read_csv("workflow/bin_configs.csv")['Config'].values

# This rule plots the number of distance computations against
# the RC metric for different datasets, coloring points
# according to their "difficulty group"
rule plot_synthetic_difficulty:
    input:
        metrics = "results/metrics/all-synthetic.csv"
    output:
        "results/plots/synthetic-difficulty.png"
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input["metrics"])
        df["target"] = df[[ "target_low", "target_high" ]].apply(lambda row: f"{row[0]}--{row[1]}", axis=1)
        averages = df.groupby(["dataset", "target", "target_metric"]).mean().reset_index()
        print(averages[["dataset", "target", "distcomp", "rc_10", "lid_10"]])

        (
          so.Plot(df, y="lid_10", x="distcomp", color="target")
            .facet("dataset")
            .add(so.Dot())
            #.add(so.Dot(color="black"), data=averages)
            .save(output[0])
        )

rule plot_synthetic_runtime:
    input:
        "results/metrics/all-synthetic-runtime.csv"
    output:
        "results/plots/synthetic-difficulty-runtime.png"
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input[0])
        df["target"] = df[[ "target_low", "target_high" ]].apply(lambda row: f"{row[0]}--{row[1]}", axis=1)
        print(df.columns)
        averages = df.groupby(["dataset", "target", "target_metric"])[["time_s"]].mean().reset_index()

        (
          so.Plot(df, y="time_s", x="target")
            .facet("dataset")
            .add(so.Dot())
            .add(so.Dot(color="black"), data=averages)
            .save(output[0])
        )


# This task computes the CSV file for the metrics
# of the queries builtin each dataset.
rule metrics_builtin_queries:
    input:
        expand(
          "results/metrics/{name}-k{k}.csv",
          name      = DATASETS,
          k         = [10]
        )
    output:
        "results/metrics/all-builtin.csv"
    threads: workflow.cores / 2
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


rule plot_ann_bench_distcomps:
    input:
        "results/metrics/all-builtin.csv"
    output:
        "results/plots/ann-benchmarks-distcomps.png"
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import seaborn.objects as so
        import pandas as pd

        df = pd.read_csv(input[0])
        print(df)

        sns.kdeplot(
            data=df,
            x = "distcomp",
            hue = "dataset",
            cut = 0
        )
        plt.tight_layout()
        plt.savefig(output[0])



# Here we compute the metrics for each individual dataset, this
# a subtask of `metrics_builtin_queries`
rule eval_metrics_builtin_queries:
    input:
        get_data_path(DATA_DIR)
    output:
        "results/metrics/{dataset}-k{k}.csv",
    run:
        import metrics
        metrics.metrics_csv(
          input[0],
          input[0],
          output[0],
          int(wildcards['k']),
          additional_header = ["dataset", "k"],
          additional_row = [wildcards['dataset'], int(wildcards['k'])]
        )


# Here we bring in a single csv file the metrics computed
# on the syntehtic query sets
rule synthetic_metrics:
    input:
        metrics = expand(
          "results/metrics/{params}.csv",
          params=paramspace.instance_patterns
        )
    output:
        "results/metrics/all-synthetic.csv"
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input["metrics"]))
        df.to_csv(output[0], index=False)


# Brings together the information from metrics and index runtime evaluation
rule eval_runtime:
    input:
        metrics = f"results/metrics/{paramspace.wildcard_pattern}.csv",
        faiss_hnsw = f"results/metrics/faiss_hnsw/{paramspace.wildcard_pattern}.csv",
    output:
        f"results/metrics/runtime/{paramspace.wildcard_pattern}.csv",
    run:
        import pandas as pd
        metrics = pd.read_csv(input["metrics"]).rename({"i": "query_index"}, axis=1)
        faiss_runtime = pd.read_csv(input["faiss_hnsw"])
        df = pd.merge(metrics, faiss_runtime)
        df.to_csv(output[0], index=False)


# Brings together all the metrics and the runtime evaluations of different indices
# FIXME: keep only one between this and synthetic_metrics
rule eval_runtime_all:
    input:
        expand(
          "results/metrics/runtime/{params}.csv",
          params=paramspace.instance_patterns
        )
    output:
        "results/metrics/all-synthetic-runtime.csv"
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


# Run faiss_hnsw on the synthetic datasets
rule faiss_hnsw_synthetic:
    input:
        data = get_data_path(DATA_DIR), #os.path.join(DATA_DIR, "{dataset}.hdf5"),
        queries =
          os.path.join(GENERATED_DIR, 
            f"{paramspace.wildcard_pattern}.hdf5"
          ),
    output:
        f"results/metrics/faiss_hnsw/{paramspace.wildcard_pattern}.csv",
    run:
        import read_data
        import faiss
        import utils
        import time
        import pandas as pd
        import json

        k = int(wildcards["k"])
        dataset, distance_metric = read_data.read_multiformat(input["data"], "train")
        queryset, _ = read_data.read_multiformat(input["queries"], "test")
        print("Populating index")
        index_params = "HNSW32"
        index = faiss.index_factory(dataset.shape[1], index_params)
        index.add(dataset)
        index.train(dataset)
        print("done")
        target_recall = 0.95

        res = []

        for q_idx in range(queryset.shape[0]):
            print("running query", q_idx)
            q = queryset[q_idx,:].reshape(1, -1)
            ground_distances = utils.compute_distances(q, None, distance_metric, dataset)[0, :]
            index.hnsw.efSearch = 1
            rec = 0
            while rec < target_recall and index.hnsw.efSearch < 10**6:
                print("  efSearch", index.hnsw.efSearch)
                dists = utils.compute_distances(q, k, distance_metric, index)[0, :]
                rec = utils.compute_recall(ground_distances, dists, k)
                if rec >= target_recall or index.hnsw.efSearch > 10**6:
                    break
                # we only increment it if we are going to do another iteration
                index.hnsw.efSearch *= 2
            print("Timing the runs")
            nruns = 10
            tstart = time.time()
            for _ in range(nruns):
                index.search(q, k)
            tend = time.time()
            elapsed_secs = ( tend - tstart ) / nruns
            res.append({
                "query_index": q_idx,
                "target_recall": target_recall,
                "index_descr": json.dumps({
                    "index_params": index_params,
                    "efSearch": index.hnsw.efSearch
                }, sort_keys=True),
                "time_s": elapsed_secs,
                "recall": rec,
            })
        df = pd.DataFrame(res)
        df.to_csv(output[0], index=False)


# Here we generate syntehtic query sets, guided by the
# relative contrast metric
rule generate_synthetic:
    input:
        #os.path.join(DATA_DIR, "{dataset}.hdf5")
        get_data_path(DATA_DIR)
    output:
        os.path.join(GENERATED_DIR, 
          f"{paramspace.wildcard_pattern}.hdf5"
        ),
    threads: workflow.cores
    params:
        paramspace.instance
    run:
        import generate
        import logging
        logging.basicConfig(level=logging.INFO)
        generate.generate_workload_annealing(
          input[0],
          output[0],
          k = int(wildcards['k']),
          metric = wildcards["difficulty"],
          target_low = float(wildcards['target_lower']),
          target_high = float(wildcards['target_upper']),
          num_queries = int(wildcards['num_queries']),
          scale = float(wildcards['scale']),
          initial_temperature = int(wildcards['initial_temperature']),
          max_steps = 10000,
          threads = threads
        )


# And finally here we compute the metrics of the synthetic
# query sets
rule eval_metrics:
    input:
        data = get_data_path(DATA_DIR), #os.path.join(DATA_DIR, "{dataset}.hdf5"),
        queries =
          os.path.join(GENERATED_DIR, 
            f"{paramspace.wildcard_pattern}.hdf5"
          ),
    output:
        f"results/metrics/{paramspace.wildcard_pattern}.csv",
    params:
        conf = paramspace.instance
    run:
        import metrics
        metrics.metrics_csv(
          input['data'],
          input['queries'],
          output[0],
          int(wildcards['k']),
          additional_header = ["dataset", "target_low", "target_high", "target_metric"],
          additional_row = [wildcards['dataset'], float(wildcards['target_lower']), float(wildcards['target_upper']), wildcards["difficulty"]]
        )

################################################################################
#
# TEST: Rules to compute all metrics and merge with messi
# ===============
#
#
################################################################################

metrics_paramspace = metrics_param_space()
# print(metrics_paramspace.to_string())
DATASETS_SIM = set(metrics_paramspace.dataframe['dataset'])
datasets_meta = dataset_param_space()
print(datasets_meta.to_string())

rule metrics_all:
    input:
        expand(
          os.path.join(RES_DIR,"metrics/messi_{dataset}_{queries}_k{k}.csv"),
          #os.path.join(RES_DIR,"metrics/{dataset}_{queries}_k{k}.csv"),
          dataset = DATASETS_SIM,
          queries = {metrics_paramspace.wildcard_pattern},
          # dataset = ["sald-128-1m"], #['fashion-mnist'],
          # queries = ["sald-128-1k"], #['fashion-mnist'],
          k = [10]
        )
    output:
        os.path.join(RES_DIR,"metrics/metrics-messi-all.csv")
        # os.path.join(RES_DIR,"metrics/metrics-all.csv")
    threads: workflow.cores / 2
    wildcard_constraints:
        k="[0-9]+"
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


rule eval_all_metrics:
    input:
        data = os.path.join(DATA_DIR, "{dataset}.bin"),  #  extend with multiple extensions
        queries = os.path.join(DATA_DIR, "{queries}.bin"),
    output:
        os.path.join(RES_DIR,"metrics/{dataset}_{queries}_k{k}.csv")
    threads: workflow.cores / 2
    run:
        import metrics
        metrics.metrics_csv(
          input['data'],
          input['queries'],
          output[0],
          int(wildcards['k']))


rule run_messi_shell:
    input:
        data = get_data_path(DATA_DIR),
        queries = get_data_path(DATA_DIR),
    output:
        temp(os.path.join(RES_DIR,"metrics/messi_{dataset}_{queries}_k{k}.txt"))
    wildcard_constraints:
        k="[0-9]+"
    params:
        sax_length = 16,
        sax_card = 8,
        leaf_size = 10000,
        cpu_cores = 4 # todo param
        ## https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#the-lookup-function ##
        #data_samples = lookup(query="dataset == '{dataset}'", within=datasets_meta ,cols=["data_samples"]),
        #q_samples = df.loc[df['dataset'] == wildcards['queries'], 'data_samples'].iloc[0],
    threads: workflow.cores / 2
    log:
         temp(os.path.join(RES_DIR,"metrics/messi_{dataset}_{queries}_k{k}.log"))
    run:
        
        data_samples = datasets_meta.loc[datasets_meta['dataset'] == wildcards['dataset'], 'data_samples'].iloc[0]
        q_samples = datasets_meta.loc[datasets_meta['dataset'] == wildcards['queries'], 'data_samples'].iloc[0]
        slength = datasets_meta.loc[datasets_meta['dataset'] == wildcards['dataset'], 'data_features'].iloc[0]

        shell("/home/qwang/projects/isax-modularized/build/isax \
--database_filepath='{input.data}' \
--query_filepath='{input.queries}' \
--database_size={data_samples} \
--query_size={q_samples} \
--sax_length={params.sax_length} \
--sax_cardinality={params.sax_card} \
--cpu_cores={params.cpu_cores} \
--log_filepath='{log}' \
--series_length={slength} \
--adhoc_breakpoints \
--leaf_size={params.leaf_size} \
--k={wildcards.k} \
--with_id ")

        shell("grep -F 'l2square' {log} | grep 'query_engine.c' | cut -d ' ' -f 8,11 >> {output} ")
        #shell("rm {log}")


# add messi output to other metrics.csv
rule merge_messi:
    input:
        messi = os.path.join(RES_DIR,"metrics/messi_{dataset}_{queries}_k{k}.txt"),
        metrics = os.path.join(RES_DIR,"metrics/{dataset}_{queries}_k{k}.csv"),
    output:
        os.path.join(RES_DIR,"metrics/messi_{dataset}_{queries}_k{k}.csv")
    wildcard_constraints:
        k="[0-9]+"
    params:
        #slength=128, #todo param
        #data_samples = 1000000,
        sax_length=16,
    run:   
        slength = datasets_meta.loc[datasets_meta['dataset'] == wildcards['dataset'], 'data_features'].iloc[0]
        data_samples = datasets_meta.loc[datasets_meta['dataset'] == wildcards['dataset'], 'data_samples'].iloc[0]
        
        l = []      
        with open(input['messi']) as f:
            for line in f.readlines():
                x, y = line.split(" ")
                # l.append(int(params['slength'])*int(x) + int(params['sax_length'])*int(y))
                c = (slength*int(x) + params['sax_length']*int(y))/(slength*data_samples)
                l.append(c)

        df = pd.read_csv(input['metrics'])    
        df['messi'] = l
        df.to_csv(output[0], index=False)


# todo: rule to add noise to queries if not exists 

################################################################################
#
# UMAP Embeddings
# ===============
#
# This is still a draft, computing the UMAP embeddings for the
# datasets, to visualize them in two dimensions.
#
################################################################################

rule umap_embedding_plots:
    input:
        expand(".data/umap/{name}.png", name=DATASETS)

rule umap_embedding_plot:
    input:
        ".data/umap/{name}.hdf5"
    output:
        ".data/umap/{name}.png",
    run:
        import h5py
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt

        with h5py.File(input[0]) as hfp:
            embedding = hfp["train"][:]
        plt.scatter(embedding[:,0], embedding[:,1], s=0.1)
        plt.tight_layout()
        plt.savefig(output[0])


rule umap_embedding:
    input:
        ".data/{name}.hdf5"
    output:
        embedding = ".data/umap/{name}.hdf5",
        embedder = ".data/umap/embedder/{name}.pkl.gz"
    threads: workflow.cores
    run:
        import umap
        import read_data
        import h5py
        import pickle
        import gzip
        embedder = umap.UMAP()
        dataset, _ = read_data.read_multiformat(input[0], "train")
        embedding = embedder.fit_transform(dataset)
        with h5py.File(output["embedding"], "w") as hfp:
            hfp["train"] = embedding
        with gzip.open(output["embedder"], "wb") as fp:
            pickle.dump(embedder, fp)


rule prepare_data_file:
    output:
        os.path.join(DATA_DIR, "{name}.hdf5")
    wildcard_constraints:
        name="[a-zA-Z0-9-]+"
    run:
        import read_data
        name = wildcards["name"]
        read_data.download(f"https://ann-benchmarks.com/{name}.hdf5", output[0])

rule run_messi_one:
	output:
		"messi_results_config{index}.txt"
	run:
		import os
		df = pd.read_csv("bin_configs.csv")
		row = wildcards.get('index')
		dsfile = df.query('Config=='+row)['DataFile'].values[0]
		queryfile = df.query('Config=='+row)['QueryFile'].values[0]
		slength=str(df.query('Config=='+row)['SeriesLength'].values[0])
		#print(slength)
		saxlength=16

		shelline = "rm messi"+row+".log"
		os.system(shelline)
		
		shelline = "rm messi_results_interm_config"+row+".txt"
		os.system(shelline)
	
		shelline = "rm messi_results_config"+row+".txt"
		os.system(shelline)

		shelline = '../isax-modularized/build/isax --database_filepath='+dsfile+' --query_filepath='+queryfile+' --database_size=1000000 --query_size=1000 --sax_length='+str(saxlength)+' --sax_cardinality=8 --cpu_cores=4 --log_filepath=messi'+str(row)+'.log --series_length='+slength+' --adhoc_breakpoints --leaf_size=10000 --k=1 --with_id'
		#print(shelline)
		os.system(shelline)
	

		shelline = "grep -F 'l2square' messi"+row+".log | grep 'query_engine.c' | cut -d ' ' -f 8,11 >> messi_results_interm_config"+row+".txt"
		os.system(shelline)

		dict = {}
		l = []
		with open("messi_results_interm_config"+row+".txt") as f:
			for line in f.readlines():
				x, y = line.split(" ")
				l.append(int(slength)*int(x) + int(saxlength)*int(y))
		dict[row] = l
        
		df = pd.DataFrame.from_dict(dict,)
		df.to_csv('messi_results_config'+row+'.txt',index=False)

	
rule run_messi_all:
	input:
		expand("messi_results_config{index}.txt", index = BIN_CONFIGS)		
	output:
		"messi_all.csv"
	run:
		for f in input:
			print(pd.read_csv(f))
		df = pd.concat((pd.read_csv(f) for f in input), axis=1)
		#print(df)
		df.to_csv(output[0], index=False)
		
		
		
		
	
