from snakemake.utils import min_version
# Here we list the minimum accepted snakemake version
min_version("7.24")

import pandas as pd
from parameters import setup_param_space

paramspace = setup_param_space()
print(paramspace.dataframe)

DATASETS = set(paramspace.dataframe['dataset'])


rule plot_syntetic_difficulty:
    input:
        metrics = "results/metrics/all-synthetic.csv"
    output:
        "results/plots/synthetic-difficulty.png"
    run:
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt
        import seaborn as sns
        import pandas as pd

        df = pd.read_csv(input["metrics"])
        df["target_rc"] = df["target_rc"].astype(str)
        averages = df.groupby(["dataset", "target_rc"]).mean()

        sns.relplot(
          data = df,
          x = "rc_10",
          y = "distcomp",
          hue = "target_rc",
          col = "dataset",
          col_wrap = 3,
          facet_kws = {"sharey": False}
        )
        plt.savefig(output[0])


rule metrics_builtin_queries:
    input:
        expand(
          "results/metrics/{name}-k{k}.csv",
          name      = DATASETS,
          k         = [10]
        )
    output:
        "results/metrics/all-builtin.csv"
    threads: workflow.cores / 2
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


rule eval_metrics_builtin_queries:
    input:
        ".data/{name}.hdf5"
    output:
        "results/metrics/{name}-k{k}.csv",
    run:
        import metrics
        metrics.metrics_csv(
          input[0],
          input[0],
          output[0],
          int(wildcards['k']),
          additional_header = ["dataset", "k"],
          additional_row = [wildcards['name'], int(wildcards['k'])]
        )


rule synthetic_metrics:
    input:
        expand(
          "results/metrics/{params}.csv",
          params=paramspace.instance_patterns
          #name      = DATASETS,
          #k         = [10],
          #target_rc = [2.0, 1.5, 1.2, 1.05],
          #num_queries = [10]
        )
    output:
        "results/metrics/all-synthetic.csv"
    run:
        import pandas as pd
        df = pd.concat((pd.read_csv(f) for f in input))
        df.to_csv(output[0], index=False)


rule generate_rc:
    input:
        ".data/{dataset}.hdf5"
    output:
        #".generated/{name}-queries{num_queries}-k{k}-rc{target_rc}.hdf5",
        f".generated/{paramspace.wildcard_pattern}.hdf5",
    threads: workflow.cores / 2
    params:
        paramspace.instance
    run:
        import generate
        generate.generate_workload(
          input[0],
          output[0],
          k = int(wildcards['k']),
          metric = wildcards["difficulty"],
          target_low = float(wildcards['target_lower']),
          target_high = float(wildcards['target_upper']),
          num_queries = int(wildcards['num_queries']),
          scale = float(wildcards['scale']),
          probes = int(wildcards['nprobes'])
        )

rule eval_metrics:
    input:
        data = ".data/{dataset}.hdf5",
        queries = f".generated/{paramspace.wildcard_pattern}.hdf5"
    output:
        #"results/metrics/{name}-queries{num_queries}-k{k}-rc{target_rc}.csv"
        f"results/metrics/{paramspace.wildcard_pattern}.csv",
    params:
        conf = paramspace.instance
    run:
        import metrics
        metrics.metrics_csv(
          input['data'],
          input['queries'],
          output[0],
          int(wildcards['k']),
          additional_header = ["dataset", "target_low", "target_high"],
          additional_row = [wildcards['dataset'], float(wildcards['target_lower']), float(wildcards['target_upper'])]
        )

################################################################################
#
# UMAP Embeddings
#
################################################################################

rule umap_embedding_plots:
    input:
        expand(".data/umap/{name}.png", name=DATASETS)

rule umap_embedding_plot:
    input:
        ".data/umap/{name}.hdf5"
    output:
        ".data/umap/{name}.png",
    run:
        import h5py
        import matplotlib
        matplotlib.use("Agg")
        from matplotlib import pyplot as plt

        with h5py.File(input[0]) as hfp:
            embedding = hfp["train"][:]
        plt.scatter(embedding[:,0], embedding[:,1], s=0.1)
        plt.tight_layout()
        plt.savefig(output[0])


rule umap_embedding:
    input:
        ".data/{name}.hdf5"
    output:
        embedding = ".data/umap/{name}.hdf5",
        embedder = ".data/umap/embedder/{name}.pkl.gz"
    threads: workflow.cores
    run:
        import umap
        import read_data
        import h5py
        import pickle
        import gzip
        embedder = umap.UMAP()
        dataset, _ = read_data.read_hdf5(input[0], "train")
        embedding = embedder.fit_transform(dataset)
        with h5py.File(output["embedding"], "w") as hfp:
            hfp["train"] = embedding
        with gzip.open(output["embedder"], "wb") as fp:
            pickle.dump(embedder, fp)


rule prepare_data_file:
    output:
        ".data/{name}.hdf5"
    wildcard_constraints:
        name="[a-zA-Z0-9-]+"
    run:
        import read_data
        name = wildcards["name"]
        read_data.download(f"https://ann-benchmarks.com/{name}.hdf5", output[0])
